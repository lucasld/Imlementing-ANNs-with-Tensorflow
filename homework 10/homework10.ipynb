{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV9apKqBrEKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e771c557-bffb-437a-a8d0-41f0312a8aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# imports\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "E02LWqeT3KP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42"
      ],
      "metadata": {
        "id": "vETSXQ9O3Ofb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100         # How many sets of words to train on at once.\n",
        "embedding_size = 100    # The embedding size of each word to train.\n",
        "\n",
        "num_sampled = int(batch_size/2) # Number of negative examples to sample.\n",
        "window_size = 2         # How many words to consider left and right."
      ],
      "metadata": {
        "id": "IWMVKNNrR9Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load Data"
      ],
      "metadata": {
        "id": "nErjwzC4dLHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bash code to mount the drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('drive/MyDrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV587j5nRhWp",
        "outputId": "0a3e6077-31e8-4de4-ea8a-e28270b47631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bible = open('bible.txt', 'r')"
      ],
      "metadata": {
        "id": "nYyETkO3TFhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = bible.read()"
      ],
      "metadata": {
        "id": "se09CNQIXfyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SORiJCkajJ_e",
        "outputId": "71f125a7-a3cb-4c9d-9041-a50dc6368003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4332496"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Word Embedding"
      ],
      "metadata": {
        "id": "MoVObqPZdOrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Preprocessing"
      ],
      "metadata": {
        "id": "Jvg5LSfwdG-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare stop words\n",
        "stops = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "2dQELtWbSXMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "e03adba6-9810-45f8-fbd4-9fb10dd9700d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bd158414d755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Declare stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'words'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeText(text):\n",
        "  # Lower Case\n",
        "  text = text.lower()\n",
        "  # Remove Linebreaks and extra whitespace\n",
        "  text = text.replace(\"\\n\\n\", \"\").replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
        "  # remove special characters\n",
        "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "  # remove numbers\n",
        "  text = re.sub(r\"[0-9]\", \"\", text)\n",
        "\n",
        "  # tokenize text\n",
        "  text = text.split()\n",
        "\n",
        "  # remove stopwords\n",
        "  #text = [w for w in text if w not in stops]\n",
        "\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "qaQtxZcS3qWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildData(text, vocab_size):\n",
        "  # Initialize list of [word, word_count] for each word, starting with unknown\n",
        "  count = [['UNK', -1]]\n",
        "  # add most frequent words, limited to the N-most frequent (N=vocabulary size)\n",
        "  count.extend(collections.Counter(text).most_common(vocab_size - 1))\n",
        "\n",
        "  word2num = {}\n",
        "  # For each word, that we want in the dictionary, add it, then make it\n",
        "  # the value of the prior dictionary length\n",
        "  for word, _ in count:\n",
        "      word2num[word] = len(word2num)\n",
        "\n",
        "  # turn the text into number data\n",
        "  data = []\n",
        "  unk_count = 0\n",
        "  for word in text:\n",
        "      if word in word2num:\n",
        "          index = word2num[word]\n",
        "      else:\n",
        "          index = 0  # word2num['UNK']\n",
        "          unk_count += 1\n",
        "      data.append(index)\n",
        "  #add count of te unkown words to the count tracker\n",
        "  count[0][1] = unk_count\n",
        "  # create a number to word dictionary\n",
        "  num2word = {index: token for token, index in word2num.items()}\n",
        "  return data, count, word2num, num2word\n",
        "\n"
      ],
      "metadata": {
        "id": "Um8Nz0-t4qbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createInputTargetPairs(data, window_size, vocab_size):\n",
        "  skipgram, label= tf.keras.preprocessing.sequence.skipgrams(\n",
        "                  sequence = data, vocabulary_size = vocab_size, \n",
        "                  window_size=window_size, negative_samples=0, shuffle=False)\n",
        "  #input, context =  list(zip(*skipgram))\n",
        "  #test = tf.reshape()\n",
        "\n",
        "  return skipgram\n",
        "  "
      ],
      "metadata": {
        "id": "sGecrE9A3gKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(raw_text, vocab_size, window_size):\n",
        "  text = normalizeText(raw_text)\n",
        "  data, count, word2num, num2word = buildData(text, vocab_size)\n",
        "  pair = createInputTargetPairs(data, window_size, vocab_size)\n",
        "  data = tf.data.Dataset.from_tensor_slices(pair)\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  data = data.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  data = data.shuffle(1000)\n",
        "  data = data.batch(50)\n",
        "  data = data.prefetch(100)\n",
        "  #return preprocessed dataset\n",
        "  return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wbrierS5JBK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = preprocessing(text, 10, 2)"
      ],
      "metadata": {
        "id": "5g15rL2gaQ5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in dataset.take(1):\n",
        "  print(pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0i6Re7n9IlI",
        "outputId": "b05b7dde-09b6-41a9-c56a-a52e79e8234d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[3 1]\n",
            " [2 3]\n",
            " [1 2]\n",
            " [1 3]\n",
            " [2 2]\n",
            " [1 2]\n",
            " [1 6]\n",
            " [3 4]\n",
            " [4 2]\n",
            " [1 3]\n",
            " [6 1]\n",
            " [1 3]\n",
            " [1 5]\n",
            " [3 1]\n",
            " [2 1]\n",
            " [7 2]\n",
            " [2 1]\n",
            " [1 3]\n",
            " [1 3]\n",
            " [2 1]\n",
            " [1 3]\n",
            " [2 8]\n",
            " [5 1]\n",
            " [1 3]\n",
            " [9 1]\n",
            " [7 5]\n",
            " [3 1]\n",
            " [1 2]\n",
            " [2 1]\n",
            " [1 2]\n",
            " [2 9]\n",
            " [2 1]\n",
            " [2 2]\n",
            " [9 7]\n",
            " [3 1]\n",
            " [2 1]\n",
            " [2 1]\n",
            " [1 2]\n",
            " [7 2]\n",
            " [2 2]\n",
            " [1 2]\n",
            " [2 4]\n",
            " [2 8]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [2 1]\n",
            " [3 1]\n",
            " [1 1]\n",
            " [2 2]\n",
            " [2 7]], shape=(50, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Model"
      ],
      "metadata": {
        "id": "sIgE4vU9aRaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, embedding_size):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.vocabulary_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "  def build(self):\n",
        "    self.embedding = self.add_weight(\n",
        "                            shape=[self.vocabulary_size, self.embedding_size],\n",
        "                            initializer = 'random_uniform', name = 'embedding')\n",
        "    self.score_w = self.add_weight(\n",
        "                            shape = [self.vocabulary_size, self.embedding_size],\n",
        "                            initializer = 'truncated_normal', name= 'score_weights')\n",
        "    self.score_b = self.add_weight(shape =[self.vocabulary_size], \n",
        "                                   initializer = 'zeros', name='score_bias')\n",
        "\n",
        "  def call(self, pair, mode = \"train\"):\n",
        "    input, context = tf.reshape(pair, shape = (2,50))\n",
        "    context = tf.reshape(context, shape= (50,1))\n",
        "    embed = tf.nn.embedding_lookup(self.embedding, input)\n",
        "    if mode == \"train\":\n",
        "      loss = tf.reduce_mean(tf.nn.nce_loss(weights = self.score_w, \n",
        "                            biases = self.score_b, \n",
        "                            labels = context, \n",
        "                            inputs = embed, \n",
        "                            num_sampled = 1,\n",
        "                            num_classes = self.vocabulary_size)\n",
        "                            )\n",
        "\n",
        "      loss_summary = tf.summary.scalar(\"loss_summary\", loss)\n",
        "      return loss_summary\n",
        "    elif mode == \"eval\":\n",
        "      out = tf.tensordot(embed, self.score_w, axes = [[1], [0]])\n",
        "      return tf.argmax(out)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiZ0skyUfwf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = SkipGram(10,10)"
      ],
      "metadata": {
        "id": "mVDa2PL92YA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer.build()\n",
        "for pair in dataset.take(1):\n",
        "  layer.call(pair, 'eval')\n"
      ],
      "metadata": {
        "id": "TmXwFi8S14eQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}