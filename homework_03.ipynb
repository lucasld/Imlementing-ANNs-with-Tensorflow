{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xoFau5W0r7f"
      },
      "source": [
        "## Homework 03"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiMIrYvB0-Wl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vozHtO20z_E"
      },
      "source": [
        "### 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrkcFSXH0wpN"
      },
      "source": [
        "# Load Bacteria Dataset from tfds\n",
        "ds_train, ds_test = tfds.load(\"genomics_ood\", split=['train[:100000]', 'test[:1000]'], shuffle_files=True, as_supervised=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5KxzPEW9m0o"
      },
      "source": [
        "def one_hot_sequence(seq: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Convert sequence of characters (A,C,G,T) to one-hot representation.\n",
        "    :param seq: sequence to translate to one-hot\n",
        "    :type seq: tf.Tensor\n",
        "    :return: one-hot tensor that is 4 times longer than the non-one-hot tensor\n",
        "    :rtype: tf.Tensor\n",
        "    \"\"\"\n",
        "    seq = tf.strings.regex_replace(seq, 'A', '0')\n",
        "    seq = tf.strings.regex_replace(seq, 'C', '1')\n",
        "    seq = tf.strings.regex_replace(seq, 'G', '2')\n",
        "    seq = tf.strings.regex_replace(seq, 'T', '3')\n",
        "    split_seq = tf.strings.bytes_split(seq)\n",
        "    int_seq = tf.cast(tf.strings.to_number(split_seq), dtype=tf.int32)\n",
        "    onehot = tf.one_hot(int_seq, 4)\n",
        "    onehot = tf.reshape(onehot, (-1,))\n",
        "    return onehot"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvzjyKUZDXHB"
      },
      "source": [
        "def preprocessing_pipeline(data):\n",
        "    \"\"\"Apply preproccesing pipeline to the given dataset.\n",
        "    \n",
        "    :param data: data to be preprocessed\n",
        "    :type data: tensorflow 'Dataset'\n",
        "    :return: preprocessed dataset\n",
        "    :rtype: tensorflow 'Dataset'\n",
        "    \"\"\"\n",
        "    # one hot the sequences and their labels\n",
        "    data = data.map(lambda seq, label: (\n",
        "        one_hot_sequence(seq),\n",
        "        tf.one_hot(label, 10)\n",
        "    ))\n",
        "    # cache the dataset\n",
        "    data = data.cache()\n",
        "    # shuffle, batch and prefetch the dataset\n",
        "    data = data.shuffle(1000)\n",
        "    data = data.batch(50)\n",
        "    data = data.prefetch(100)\n",
        "    return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50IDryVDJwBF"
      },
      "source": [
        "Apply the preprocessing pipeline to both the training data and the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2idkLL5t5APt"
      },
      "source": [
        "ds_train = ds_train.apply(preprocessing_pipeline)\n",
        "ds_test = ds_test.apply(preprocessing_pipeline)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nIeVBlpKSHk"
      },
      "source": [
        "### 2. Model\n",
        "Layer and Model Class should be implemented by hand using the [*tf.keras.layers.Layer*](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) and the [*tf.keras.Model*](https://www.tensorflow.org/api_docs/python/tf/keras/Model) classes.\\\n",
        "Our network will have the following layers:\n",
        "* Hidden Layer 1: 256 units - Sigmoid Activation\n",
        "* Hidden Layer 2: 256 units - Sigmoid Activation\n",
        "* Ouput Layer: 10 units - Softmax Activation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ1DVkT99PgA"
      },
      "source": [
        "class CustomDense(tf.keras.layers.Layer):\n",
        "    \"\"\"This is a custom Dense Layer class\n",
        "    \n",
        "    :param units: number of perceptrons in the layer\n",
        "    :type units: integer\n",
        "    :param activation: activation function that the perceptrons use\n",
        "    :type activation: tf.nn activation function\n",
        "    \"\"\"\n",
        "    def __init__(self, units, activation):\n",
        "        \"\"\"Constructor function\"\"\"\n",
        "        super(CustomDense, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build function runs once when first pass is executed. This way the\n",
        "        input shape is dynamic.\n",
        "\n",
        "        :param input_shape: shape of the input that feeds into the layer\n",
        "        :type input_shape: tf.Tensor\n",
        "        \"\"\"\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \"\"\"Compute forward pass through layer.\n",
        "\n",
        "        :param inputs: Layer inputs\n",
        "        :type inputs: tf.Tensor\n",
        "        :return: Layer outputs\n",
        "        :rtype: tf.Tensor\n",
        "        \"\"\"\n",
        "        x = tf.matmul(inputs, self.w) + self.b\n",
        "        x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzaIeeudWW87"
      },
      "source": [
        "class CustomModel(tf.keras.Model):\n",
        "    \"\"\"This is a custom model class\n",
        "    \n",
        "    :param loss_function: loss function used to calculate loss of the model\n",
        "    :type loss_function: function from the tf.keras.losses module\n",
        "    :param optimizer: optimizer used to apply gradients to the models\n",
        "        trainable variables\n",
        "    :type optimizer: function from the tf.keras.optimizers module\n",
        "    :param layers: Contains all Layers of the model, defaults to 3 layers:\n",
        "        1. CustomDense(256, activation=tf.nn.sigmoid)\n",
        "        2. CustomDense(256, activation=tf.nn.sigmoid)\n",
        "        3. CustomDense(10, activation=tf.nn.softmax)\n",
        "    :type layers: list of CustomDense-Objects, optional\n",
        "    \"\"\"\n",
        "    def __init__(self, loss_function, optimizer,\n",
        "                 layer_list=[CustomDense(256, activation=tf.nn.sigmoid),\n",
        "                             CustomDense(256, activation=tf.nn.sigmoid),\n",
        "                             CustomDense(10, activation=tf.nn.softmax)]):\n",
        "        \"\"\"Constructor function\"\"\"\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.layer_collection = layer_list\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Compute the feed-forward pass through all dense layers.\n",
        "        \n",
        "        :param inputs: network input\n",
        "        :type inputs: tf.Tensor\n",
        "        \"\"\"\n",
        "        x = inputs\n",
        "        for layer in self.layer_collection:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "    \n",
        "    def train_step(self, input, target):\n",
        "        \"\"\"Applys optimizer to all trainable variables of this model to\n",
        "        minimize the loss (loss_function) between the target output and the\n",
        "        predicted ouptut.\n",
        "\n",
        "        :param input: input to the model\n",
        "        :type input: tf.Tensor\n",
        "        :param target: target output with repect to the input\n",
        "        :type target: tf.Tensor\n",
        "        :return: the loss and the accuracy of the models prediction\n",
        "        :rtype: tuple of two floats\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction = self(input)\n",
        "            loss = self.loss_function(target, prediction)\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        # apply gradients to the trainable variables using a optimizer\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        accuracy = self.calc_accuracy(prediction, target)\n",
        "        return loss, accuracy\n",
        "    \n",
        "    def test(self, test_data):\n",
        "        \"\"\"Calculate the mean loss and accuracy of the model over all elements\n",
        "        of test_data.\n",
        "\n",
        "        :param test_data: model is evaulated for test_data\n",
        "        :type test_data: tensorflow 'Dataset'\n",
        "        :return: mean loss and mean accuracy for all datapoints\n",
        "        :rtype: tuple of two floats\n",
        "        \"\"\"\n",
        "        # aggregator lists for tracking the loss and accuracy\n",
        "        test_accuracy_agg = []\n",
        "        test_loss_agg = []\n",
        "        # iterate over all input-target pairs in test_data\n",
        "        for (input, target) in test_data:\n",
        "            prediction = self(input)\n",
        "            loss = self.loss_function(target, prediction)\n",
        "            accuracy = self.calc_accuracy(prediction, target)\n",
        "            # add loss and accuracy to aggregators\n",
        "            test_loss_agg.append(loss.numpy())\n",
        "            test_accuracy_agg.append(np.mean(accuracy))\n",
        "        # calculate mean loss and accuracy\n",
        "        test_loss = tf.reduce_mean(test_loss_agg)\n",
        "        test_accuracy = tf.reduce_mean(test_accuracy_agg)\n",
        "        return test_loss, test_accuracy\n",
        "    \n",
        "    def calc_accuracy(self, pred, target):\n",
        "        \"\"\"Calucalte accuracy between a prediction and a target.\n",
        "\n",
        "        :param pred: a prediction that the model made\n",
        "        :type pred: tf.Tensor of floats\n",
        "        :param target: target that model should have predicted\n",
        "        :type target: tf.Tensor of floats\n",
        "        \"\"\"\n",
        "        same_prediction = tf.argmax(target, axis=1) == tf.argmax(pred, axis=1)\n",
        "        return np.mean(same_prediction)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwZy69hqqbf"
      },
      "source": [
        "### 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-CogL-pWo_6",
        "outputId": "8211a193-e4ac-4472-af1f-55a42b0ff5d4"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Setting Hyperparameters\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.1\n",
        "\n",
        "# Initialize the loss-function\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(LEARNING_RATE)\n",
        "# Initialize the model\n",
        "model = CustomModel(cross_entropy_loss, optimizer)\n",
        "\n",
        "# Initialize lists for tracking loss and accuracy\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Testing models performance before training starts.\n",
        "# Test-Dataset\n",
        "test_loss, test_accuracy = model.test(ds_test)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "# Train-Dataset\n",
        "train_loss, train_accuracy = model.test(ds_train)\n",
        "train_losses.append(train_loss)\n",
        "train_accuracies.append(train_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Training for EPOCHS.\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f'Epoch {str(epoch)} starting with test-accuracy of {np.round(test_accuracies[-1],3)}')\n",
        "    epoch_loss_agg = []\n",
        "    epoch_accuracy_agg = []\n",
        "    for input, target in ds_train:\n",
        "        train_loss, train_accuracy = model.train_step(input, target)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "        epoch_accuracy_agg.append(train_accuracy)\n",
        "    \n",
        "    # track training loss and accuracy\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "    train_accuracies.append(tf.reduce_mean(epoch_accuracy_agg))\n",
        "    # track loss and accuracy for test-dataset\n",
        "    test_loss, test_accuracy = model.test(ds_test)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 starting with test-accuracy of 0.093\n",
            "Epoch 2 starting with test-accuracy of 0.343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qApzC0IxGnW"
      },
      "source": [
        "### 4. Visualisation\n",
        "On the left plot the losses for the trainset and the testset are displayed. On the right plot the accuracy for the testset is shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXg9V392g1EN"
      },
      "source": [
        "fig, axs = plt.subplots(1, 2)\n",
        "fig.set_size_inches(20, 6)\n",
        "\n",
        "fig.suptitle('Training Progress for Genomics Bacteria Classification')\n",
        "axs[0].plot(train_losses, color='orange', label='train losses')\n",
        "axs[0].plot(test_losses, color='green', label='test losses')\n",
        "axs[0].set(ylabel='Losses')\n",
        "axs[0].legend()\n",
        "axs[1].plot(test_accuracies, color='orange', label='test accuracies')\n",
        "axs[1].plot(train_accuracies, color='green', label='train accuracies')\n",
        "axs[1].set(xlabel='Epochs', ylabel='Accuracies')\n",
        "axs[1].legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}