{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58741378-39f6-481d-8216-01759da36708",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96ce37-d4aa-46e4-ab75-371d3921fbda",
   "metadata": {},
   "source": [
    "### 1. Preparation\n",
    "Implement a function **sigmoid(x)** and a function **sigmoidprime(x)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d27b285-c042-45c7-99f9-e0ec5c51b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidprime(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b282c66-7e03-4fc0-aed5-5ae1d9890e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x, y):\n",
    "    # return mean squared error between x and y\n",
    "    return sum(map(lambda a: (a[0] - a[1])**2, zip(x, y)))/len(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4b09e-8844-4d46-8716-5708204e09d0",
   "metadata": {},
   "source": [
    "### 2.  Data Sets\n",
    "We are training the network on logical gates (**and, or, not and, not or, xor == exclusive or**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a94180bc-723e-4a68-bed3-24e3a9aff23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "labels = {\n",
    "    'AND': [0, 0, 0, 1],\n",
    "    'OR': [0, 1, 1, 1],\n",
    "    'NAND': [1, 1, 1, 0],\n",
    "    'NOR': [1, 0, 0, 0],\n",
    "    'XOR': [0, 1, 1, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b502ea-511d-45a8-85c5-979b67769669",
   "metadata": {},
   "source": [
    "### 3. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcc49b3b-ad2c-46bb-81f6-94344161baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"This is a perceptron class.\n",
    "    \n",
    "    :param input_units: number of incoming connections. The bias will be added\n",
    "        internally and doesn't have to be considered.\n",
    "    :type input_units: integer\n",
    "    :param alpha: alpha is the learning rate which determines the step size in\n",
    "        gradient direction\n",
    "    :type alpha: float\n",
    "    :param activation_func: function that should be applied to the weighted sum\n",
    "        of the inputs, defaults to sigmoid\n",
    "    :type activation_func: function, optional\n",
    "    :param activation_func_prime: derivative of the activation function, defaults\n",
    "        to the derivative of sigmoid\n",
    "    :type activation_func_prime: function, optional\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, alpha, activation_func=sigmoid,\n",
    "                 activation_func_prime=sigmoidprime):\n",
    "        \"\"\"Constructor function\"\"\"\n",
    "        # Set weights by drawing samples from the normal distribution.\n",
    "        # To account for the bias we add one more weight to the perceptron.\n",
    "        self.weights = np.random.normal(size=input_units + 1)\n",
    "        self.act_func = activation_func\n",
    "        self.act_func_prime =  activation_func_prime\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"Passes the inputs through this Perceptron\n",
    "        \n",
    "        :param inputs: array of inputs for the perceptron\n",
    "        :type inputs: numpy.array\n",
    "        :return: activated, weighted sum of the inputs\n",
    "        :rtype: numpy.array\n",
    "        \"\"\"\n",
    "        # One is added at the end of the array to account for the bias\n",
    "        self.inputs = np.append(x, [1])\n",
    "        inp =  self.weights @ self.inputs\n",
    "        self.out = self.act_func(inp)\n",
    "        return self.out\n",
    "\n",
    "    def update(self, delta):\n",
    "        \"\"\"Updates the weights of this perceptron\n",
    "        \n",
    "        :param delta:\n",
    "        :type delta: float\n",
    "        \"\"\"\n",
    "        self.weights -= self.inputs * self.alpha * delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda6ba6-f164-45ca-9d13-90866332cf1d",
   "metadata": {},
   "source": [
    "### 4. Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19aba9af-6ea8-49a3-be31-2ba9d9161b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"This is a Multi-Layer Perceptron Class.\n",
    "    \n",
    "    :param dim: array that describes the dimensions of the MLP, defaults to (2,4,1)\n",
    "    :type dim: list / tuple, optional\n",
    "    :param alpha: specifies the learning rate of the MLP, defaults to 1\n",
    "    :type alpha: float, optional\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=(2,4,1), alpha=1):\n",
    "        \"\"\"Constructor function\"\"\"\n",
    "        self.layers = []\n",
    "        for i, perceptron_amount in enumerate(dim[1:]):\n",
    "            # A layer consits of perceptron_amount perceptrons. Each perceptron receives as many\n",
    "            # weights as there are percpetrons in the previous layer equiv. i.\n",
    "            weights_in = dim[i]\n",
    "            layer = [Perceptron(weights_in, alpha) for _ in range(perceptron_amount)]\n",
    "            # add layer to layers-list\n",
    "            self.layers.append(layer)\n",
    "        # Variable to store the activations during forward propagation\n",
    "        self.net_activations = []\n",
    "\n",
    "    def forward_step(self, X):\n",
    "        \"\"\"Feeds the input X into the network and returns the final prediction.\n",
    "        \n",
    "        :param X: Input for which the MLP should create a prediction\n",
    "        :type X: np.array\n",
    "        :return: predictied label for X\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        layer_activations = X\n",
    "        for layer in self.layers:\n",
    "            # compute activations of the perceptrons in the current layer\n",
    "            layer_activations = np.array([perceptron.forward_pass(layer_activations) for perceptron in layer])\n",
    "            # save the activations for the backpropagation\n",
    "            self.net_activations.append(layer_activations)\n",
    "        return layer_activations\n",
    "    \n",
    "    def backprop_step(self, target):\n",
    "        \"\"\"Update weights using gradient descent to minimize loss. This function should be called after running the forward step.\n",
    "        \n",
    "        :param target: target label for the input provided in the forward step.\n",
    "        :type target: np.array / list\n",
    "        \"\"\"\n",
    "        # convert target to numpy array\n",
    "        target = np.array(target)\n",
    "        # check that target has the right shape\n",
    "        assert target.shape != self.net_activations[-1].shape, \"The provided target has a different shape than the network output\"\n",
    "        # calculate partial derivative of total error (MSE) with repect to the network output\n",
    "        error = - (target - self.net_activations[-1])\n",
    "        # propagate error backwards through the network\n",
    "        for layer_i, layer in enumerate(self.layers[::-1]):\n",
    "            # multiply error by the partial derivative of the activations with respect to the total perceptrons input\n",
    "            delta = error * self.net_activations[-layer_i-1] * (1 - self.net_activations[-layer_i-1])\n",
    "            # calc error for next layer\n",
    "            error = delta @ np.array([perceptron.weights[:-1] for perceptron in layer])\n",
    "            # update weights using delta\n",
    "            for i, perceptron in enumerate(layer):\n",
    "                perceptron.update(delta[i])       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6bf78-fe02-4c2f-9439-bef8773256cf",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "940fac8e-b504-4a9b-8222-f4598f387fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multi-layer perceptron\n",
    "net = MLP()\n",
    "# choose logical gate that the network should be trained to estimate\n",
    "logical_gate = 'XOR'\n",
    "\n",
    "EPOCHS = 1000\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    loss = 0\n",
    "    correct_predictions = 0\n",
    "    # choose a random order to feed the inputs to the net\n",
    "    order = np.random.permutation(4)\n",
    "    for i in order:\n",
    "        # one of the four inputs\n",
    "        X = possible_inputs[i]\n",
    "        # corresponding output\n",
    "        y = labels[logical_gate][i]\n",
    "        # perform forward propagation\n",
    "        y_hat = net.forward_step(possible_inputs[i])\n",
    "        # perform backward propagation\n",
    "        net.backprop_step(y)\n",
    "        # performance assesment\n",
    "        correct_predictions += 1 if abs(y-y_hat) < 0.5 else 0\n",
    "        loss += mean_squared_error([y], y_hat)\n",
    "\n",
    "    losses.append(loss/4)\n",
    "    accuracies.append(correct_predictions / 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115f958-ed8f-48d0-be2f-15bdb007826c",
   "metadata": {},
   "source": [
    "### 6. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44fe37a2-a564-4f4d-9699-2ccd4436dfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd23a9036c94f288343606097fa0a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Epochs'), Text(0, 0.5, 'Accuracies')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, sharex=True)\n",
    "fig.set_size_inches(10, 5)\n",
    "fig.suptitle(f'Training Progress for {logical_gate}')\n",
    "axs[0].plot(losses, color='orange')\n",
    "axs[0].set(ylabel='Losses')\n",
    "axs[1].plot(accuracies, color='green')\n",
    "axs[1].set(xlabel='Epochs', ylabel='Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30415976-ff06-41e3-b7fc-e18484af02aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
